{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Элеметы теории статистического обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Задача:\n",
    "\n",
    "Имется две пары игральных костей. Одна стандартная (на гранях числа от 1 до 6), а вторая \"подправленная\", где на каждой грани записано число на два больше (т.е. от 3 до 9). Я случайно с равной вероятностью выбираю или первую пару костей или вторую, а затем бросаю их. Вам сообщаю число выпавших очков, ваша задача - угадать, какую пару костей я подбросил стандартную или \"подправленную\". Если угадываете - получаете 10 рублей, не угадываете - вы платите мне 10 рублей :))\n",
    "\n",
    "Допустим, я сообщаю, что выпало 8 очков. На что вы поставите?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ответ\n",
    "<img src=\"img/lecBayes/DiceProbas.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    " 2   1,1                          -\n",
    " 3   1,2 2,1                      - \n",
    " 4   1,3 2,2 3,1                  -\n",
    " 5   1,4 2,3 3,2 4,1              -\n",
    " 6   1,5 2,4 3,3 4,2 5,1          3,3\n",
    " 7   1,6 2,5 3,4 4,3 5,2 1,6      3,4 4,3\n",
    " 8   2,6 3,5 4,4 5,4 6,2          3,5 4,4 5,4\n",
    " 9   3,6 4,5 5,4 6,3              3,6 4,5 5,4 6,3\n",
    "10   4,6 5,5 6,4                  3,7 4,6 5,5 6,4 7,3\n",
    "11   5,6 6,5                      3,8 4,7 5,6 6,5 7,4 8,3\n",
    "12   6,6                          4,8 5,7 6,6 7,5 8,4\n",
    "13   -                            5,8 6,7 7,6 8,5 \n",
    "14   -                            6,8 7,7 8,6\n",
    "15   -                            7,8 8,7\n",
    "16   -                            8,8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/lecBayes/DiceProbas1.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Формализация\n",
    "\n",
    "* Имеется несколько типов (классов) объектов (ситуаций): $1, 2, \\dots, K$.\n",
    "* Каждый объект (ситуация) описывается некоторым набором параметров, которые формируют вектор признаков: $X = (x_1, x_2, \\dots, x_p) \\in \\mathscr{X}$. (Обычно $\\mathscr{X}$ -- подмножество $\\mathbb{R}^p$).\n",
    "* Пропорция каждого класса $k$ в общей \"популяции\" объектов: $\\pi_k$ (может быть известна заранее).\n",
    "* Объекты каждого класса $k$ распределены в соответствии с плотностью распределения (вероятностью) $p_k(x)$, которая является функцией от $x$.\n",
    "\n",
    "*Задача:* классифицировать предъявляемый классификатору объект. Т.е. глядя на вектор признаков объекта $X=x$ вернуть одно из $K+2$ возможных решений: $1, 2, \\dots, K, \\mathscr{D}, \\mathscr{O}$ (где $\\mathscr{D}$ - отказ от классификации, $\\mathscr{O}$ - выброс, т.е. объект точно не принадлежит ни одному из $K$ классов).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Простейший случай: пропорции классов $\\pi_k$ известны, выбросы не учитываются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Предположим, мы построили два классификатора. Как узнать, который из них лучше? Какие параметры нам понадобится знать, чтобы ответить на вопрос о качестве?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Нам понадобятся: \n",
    "* вероятность ошибки классификатора для каждого класса ($\\hat{c}(X)$ -- какой ответ выдает классификатор для объекта с вектором признаков $X$): \n",
    "\\begin{equation*}\n",
    "    p_{er}(k) = P\\{\\hat{c}(X) \\neq k, \\quad \\hat{c}(X)\\in \\{1, 2, \\dots, K\\} \\quad |\\quad C=k \\}\n",
    "\\end{equation*}\n",
    "* вероятность отказа от классификации:\n",
    "\\begin{equation*}\n",
    "    p_{d}(k) = P\\{\\hat{c}(X) = \\mathscr{D} \\quad | \\quad C=k \\}\n",
    "\\end{equation*}\n",
    "* величина проигрыша при неверно выбранном ответе (функция потерь)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Функция потерь (loss function)\n",
    "\n",
    "Пусть $L(k, l)$ -- величина потерь при ответе \"объект принадлежит классу $l$\", если на самом деле объект принадлежит классу $k$. (Логично допустить: $L(k,k) = 0$).\n",
    "\\begin{equation*}\n",
    "    L(k,l)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            0, & \\mbox{если } l= k \\mbox{ (корректный ответ)}\\\\\n",
    "            1, & \\mbox{если } l\\neq k \\mbox{ и } l\\in \\{1,2,\\dots,K\\} \\\\\n",
    "            d, & \\mbox{если } l=\\mathscr{D} \\mbox{ (в затруднении)}.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Функция риска\n",
    "\n",
    "У нас есть классификатор $\\hat{c}$, нужно оценить, насколько много он \"проигрывает\" в случае, если правильный ответ $C=k$, т.е. его ожидаемые потери $R(\\hat{c}, k)$.\n",
    "\n",
    "Ожидаемая величина потерь для классификатора $\\hat{c}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    R(\\hat{c}, k) = E[L(k, \\hat{c}(X)) \\ | \\  C=k] = \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "    = \\sum_{l=1}^K L(k, l) P\\{\\hat{c}(X) =l \\ | \\ C=k\\} + L(k, \\mathscr{D}) P\\{\\hat{c}(X)=\\mathscr{D} \\ | \\ C=k\\})\n",
    "\\end{equation*}\n",
    "\n",
    "Мы несколько упростили жизнь тем, что функция потерь $L(k, l)$ одинакова для разных ошибок, пэтому:\n",
    "\\begin{equation*}\n",
    "  R(\\hat{c}, k) = p_{er}(k) + d p_d(k).\n",
    "\\end{equation*}\n",
    "\n",
    "Общий риск когда ожидаемый класс $C$ и вектор $X$ случайны:\n",
    "\n",
    "\\begin{equation*}\n",
    "   R(\\hat{c}) = E [R(\\hat{c}, C)] = \\sum_{k=1}^k \\pi_k p_{er}(k) + d \\sum_{k=1}^k \\pi_k p_d(k).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Пусть мы взяли некоторый объект и измерили его характеристики $X=(x_1, x_2, \\dots, x_p)$. \n",
    "\n",
    "\\begin{equation*}\n",
    "   p(k \\ | \\ x) = P\\{C=k \\ | \\ X=x\\} = \\frac{\\pi_k p_k(x)}{\\sum_{l=1}^K \\pi_j p_l(x)}\n",
    "\\end{equation*}\n",
    "\n",
    "<img src=\"img/lecBayes/DiceProbas1.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Правило классификации\n",
    "Для минимизации риска \n",
    "\\begin{equation*}\n",
    "    L(k,l)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            0, & \\mbox{если } l= k \\mbox{ (корректный ответ)}\\\\\n",
    "            1, & \\mbox{если } l\\neq k \\mbox{ и } l\\in \\{1,2,\\dots,K\\} \\\\\n",
    "            d, & \\mbox{если } l=\\mathscr{D} \\mbox{ (в затруднении)}.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}\n",
    "\n",
    "нужно действовать по правилу:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "    c(x)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            k, & \\mbox{если } p(k|x) = \\max_{l \\leq K} \\mbox{ и это больше } 1-d\\\\\n",
    "            \\mathscr{D}, & \\mbox{если каждый} p(k|x) \\leq 1-d.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Для минимизации риска в случае, когда потери для разных классов не равны, нужно действовать по правилу:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "    c(x)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            k, & \\mbox{если это дает минимум }  min_{l\\leq K} \\sum L(j, l) p(j | x) < d\\\\\n",
    "            \\mathscr{D}, & \\mbox{иначе}.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <<Проклятие размерности>>\n",
    "### Эффект Хьюза (Hughes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Есть (квантованное) пространство признаков, состоящее из $n$ элементов.\n",
    "* В этом пространстве \n",
    " 1. Строим все возможные пары классов в этом пространстве, \n",
    " 2. Строим классификаторы для разделения этих классов,\n",
    " 3. Вычисляем среднюю точность классификаторов.\n",
    "\n",
    "<img src=\"img/lecBayes/Hughes0.png\" height=\"30%\">\n",
    "Hughes G. On the mean accuracy of statistical pattern recognizers //IEEE transactions on information theory. – 1968\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Бесконечная обучающая выборка\n",
    "Точность классификатора напрямую зависит от $n$: чем больше признаков, тем точнее результат (но существует предел).\n",
    "<img src=\"img/lecBayes/Hughes1.png\" height=\"30%\">\n",
    "Hughes G. On the mean accuracy of statistical pattern recognizers //IEEE transactions on information theory. – 1968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Конечная обучающая выборка\n",
    "Зависимость точности от размера обучающей выборки (для случая равновероятных классов).\n",
    "<img src=\"img/lecBayes/Hughes2.png\" height=\"30%\">\n",
    "Hughes G. On the mean accuracy of statistical pattern recognizers //IEEE transactions on information theory. – 1968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Почему при конечной выборке сначала точность классификаторов увеличивается, а затем после некоторого предела падает?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Две противоположные тенденции:\n",
    " * с увеличением размерности разделимость классов также увеличивается (может увеличиться);\n",
    " * чем больше размерность, тем больше параметров классификатора нужно оценить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Отбор признаков\n",
    "\n",
    "Как оценить качество набора признаков?\n",
    "\n",
    "<img src=\"img/lecBayes/DiceProbas.png\" height=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Дивергенция - мера несходства между двумя классами\n",
    "<img src=\"img/lecBayes/Divergence.png\" height=\"30%\">\n",
    "\n",
    "Отношение плотностей и логарифм отношения:\n",
    "$$\n",
    "L_{ij} = \\frac{p(X|C=i)}{p(X|C=j)}; \\qquad L'_{ij} = \\ln [p(X|C=i)] - \\ln[p(X|C=j)]\n",
    "$$\n",
    "\n",
    "Дивергенция:\n",
    "\n",
    "$$\n",
    "D_{ij} = E\\big (L'_{ij}| C=i \\big) + E\\big ( L'_{ji} | C=j\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(где $E\\big (L'_{ij}| C=1 \\big) = \\int_x p(x|C=i) \\ln \\frac{p(x|C=i)}{p(x|C=j)} dx $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VC-размерность \n",
    "#### [Размерность Вапника-Червоненкиса](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%92%D0%B0%D0%BF%D0%BD%D0%B8%D0%BA%D0%B0_%E2%80%94_%D0%A7%D0%B5%D1%80%D0%B2%D0%BE%D0%BD%D0%B5%D0%BD%D0%BA%D0%B8%D1%81%D0%B0)\n",
    "\n",
    "* Есть два класса.\n",
    "* Мы хотим построить классификатор, которые разделяет выбранные классы.\n",
    "\n",
    "Вопрос <<Сколько параметров должно быть у классификатора, чтобы различать классы>> тесно связан с понятием VC-измерения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Дихотомия\n",
    "\n",
    "Задача разделения двух классов $C_0$ и $C_1$ в пространстве $\\mathscr{X} \\subset \\mathbb{R}^m$. Обозначим через $\\mathbb{F}$ множество дихотомий, реализованных обучающей машиной:\n",
    "\n",
    "$$\n",
    "\\mathbb{F} = \\{F(x, w): w\\in\\mathbb{W}, F: \\mathscr{X}\\times\\mathbb{W} \\to \\{0, 1\\} \\}\n",
    "$$\n",
    "\n",
    "Пусть $\\mathbb{L}$ -- множество из $N$ точек $m$-мерного пространства $\\mathscr{X}$\n",
    "$$\n",
    "\\mathbb{L} = \\{x_i \\in \\mathscr{X}; i=1, 2, \\dots, N\\}\n",
    "$$\n",
    "\n",
    "Дихотомия, реализованная обучаемой машиной разбивает $\\mathbb{L}$ на два непересекающихся подмножества $\\mathbb{L}_0$ и $\\mathbb{L}_1$:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\tF(x, w) = \t\\left\\lbrace \n",
    "\t\t\t\t\\begin{array}{rl}\n",
    "\t\t\t\t\t0, & \\mbox{ для } x\\in \\mathbb{L}_0\\\\\n",
    "\t\t\t\t\t1, & \\mbox{ для } x\\in \\mathbb{L}_1\n",
    "\t\t\t\t\\end{array}  \\right. .\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"img/lecBayes/Dichotomy.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### VC-размерность\n",
    "\n",
    "Определение:\n",
    "\n",
    "**VC-размерностью называется мощность наибольшего множества $\\mathbb{L}$, разбиением которого является $\\mathbb{F}$**\n",
    "\n",
    "Другими словами VC-размерностью называют самое большое значение $N$ для которого машина может реализовать все $2^N$ разбиений.\n",
    "\n",
    "<img src=\"img/lecBayes/Dichotomy1.png\" height=\"30%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Применение к нейросетям.\n",
    "\n",
    "* Пусть решающее правило задается формулой \n",
    "$$F: y = \\phi (w\\times x + b)$$ где $w$ -- $m$-мерный вектор весов, $b$ -- порог, функция активации пороговая \n",
    "$$\\phi (u)= \\left\\lbrace \n",
    "\t\t\t\t\\begin{array}{rl}\n",
    "\t\t\t\t\t1, & \\mbox{если $u \\geq 0$}\\\\\n",
    "\t\t\t\t\t-1, & \\mbox{если u<0}\n",
    "\t\t\t\t\\end{array}  \\right.$$\n",
    "Тогда VC-размерность правила равна $m+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Пусть NN -- произвольная нейросеть прямого распространения, состоящая из нейронов с пороговой функцией активации. Тогда VC-размерность нейросети составляет $O(W \\log W)$, где $W$ - общее количество свободных параметров сети. (Baum E. B., Haussler D. What size net gives valid generalization? //Advances in neural information processing systems. – 1989.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Пусть NN -- произвольная нейросеть прямого распространения, состоящая из нейронов с сигмоидальной функцией активации\n",
    "\\begin{equation}\n",
    "    f(u) = \\frac{1}{1+e^{-au}},\n",
    "\\end{equation}\t\n",
    "Тогда VC-размерность нейросети составляет $O(W^2)$, где $W$ - общее количество свободных параметров сети. (Koiran P., Sontag E. D. Neural networks with quadratic VC dimension //Advances in neural information processing systems. – 1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Задача"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "url=\"http://mlr.cs.umass.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "abalone = pd.read_csv(url, header=None, \n",
    "    names=['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', \n",
    "          'Viscera_weight', 'Shell_weight', 'Rings']\n",
    ")\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sample = abalone.loc[ ~ (abalone['Sex'] =='M')]\n",
    "sample = sample.copy()\n",
    "\n",
    "sample['Class'] = 0\n",
    "sample.loc[(sample['Sex'] =='I'), 'Class'] = 1.0\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plus = sample.loc[(sample['Class'] == 1.0)]\n",
    "minus = sample.loc[(sample['Class'] == 0)]\n",
    "minus.head()\n",
    "\n",
    "plus = np.array(plus)[:, 1:9]\n",
    "minus = np.array(minus)[:, 1:9]\n",
    "\n",
    "print(plus.shape)\n",
    "print(minus.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "grid = GridSearchCV(KernelDensity(),\n",
    "                        {'bandwidth': np.linspace(0.001, 0.1, 50)},\n",
    "                        n_jobs=-1,\n",
    "                        cv=10) # 10-fold cross-validation\n",
    "\n",
    "grid.fit(plus)\n",
    "print('Best bandwidth (plus):', grid.best_params_)\n",
    "kde_plus = grid.best_estimator_\n",
    "\n",
    "grid.fit(minus)\n",
    "print('Best bandwidth (minus):', grid.best_params_)\n",
    "kde_minus = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "density_plus_p = kde_plus.score_samples(plus)\n",
    "density_plus_m = kde_plus.score_samples(minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "density_minus_p = kde_minus.score_samples(plus)\n",
    "density_minus_m = kde_minus.score_samples(minus)\n",
    "\n",
    "true_plus = density_plus_p > density_minus_p\n",
    "true_plus_prop =  1.0 * sum(true_plus.astype(np.int))/len(true_plus)\n",
    "\n",
    "true_minus = density_minus_m > density_plus_m\n",
    "true_minus_prop = 1.0 * sum(true_minus.astype(np.int)) / len(true_minus)\n",
    "\n",
    "print('True plus:', true_plus_prop)\n",
    "print('True minus:', true_minus_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plus_prob = 1342/ (1342+1307)\n",
    "print(plus_prob)\n",
    "\n",
    "minus_prob = 1307/ (1342+1307)\n",
    "print(minus_prob)\n",
    "\n",
    "true_plus_prop*plus_prob + true_minus_prop*minus_prob"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
