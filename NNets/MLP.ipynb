{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Многослойный перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![Многослойный перцептрон](img/lec01/net.png)\n",
    "\n",
    "Многослойный перцептрон состоит из нескольких слоев нейронов (входной слой (нулевой), первый скрытый слой, второй скрытый слой, ..., выходной слой).\n",
    "\n",
    "* нейроны первого слоя получают входные сигналы, преобразуют их и передают нейронам второго слоя;\n",
    "* далее срабатывает второй слой, получающий сигналы от первого, этот слой также производит преобразование сигналов и их дальнейшую передачу третьему слою и т.д.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Особенности многослойного перцептрона\n",
    "\n",
    "Обычно:\n",
    "\n",
    "* Каждый нейрон имеет нелинейную функцию активации (почему?).\n",
    "* Функция активации является всюду дифференцируемой.\n",
    "* Несколько скрытых слоев.\n",
    "\n",
    "В первых работах очень часто использовалась сигмоидальная функция активации:\n",
    "\\begin{equation}\n",
    "    f(u) = \\frac{1}{1+ e^{-u}}\n",
    "\\end{equation}\n",
    "\n",
    "Сейчас часто используется ReLU:\n",
    "\\begin{equation}\n",
    "y = f(u)= \\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "        0, & \\mbox{если $u < 0$}\\\\\n",
    "        u, & \\mbox{если $u \\geq 0$}\n",
    "    \\end{array}   \\right. \n",
    "\\end{equation}\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Обучение многослойного перцептрона\n",
    "\n",
    "Вспоминаем, как производилось обучение однослойного перцептрона.\n",
    "\n",
    "<img src=\"img/oneLayer/net.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обучение однослойной сети\n",
    "\n",
    "* Обучение с учителем.\n",
    "* Возьмем $L$ примеров из задачника. Рассчитаем для них выходы. Далее можно рассчитать общую ошибку:\n",
    "\\begin{equation}\n",
    "    E = \\sum_{i=1}^{L} E(i)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E(i) = \\frac{1}{2}\\sum_{i=1}^{L} \\left( y(i) - \\tilde y (i)\\right)^2\n",
    "\\end{equation}\n",
    "где $y(i)$ -- выходное значение нейрона для примера № $i$, $\\tilde y (i)$ -- желаемое значение (эталон) для примера $i$.\n",
    "* Процедура обучения состоит в пошаговом подборе весов нейрона таким образом, чтобы ошибка $E$ уменьшалась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обучение однослойной сети: дельта-правило\n",
    "\n",
    "Пусть $w=(w_0,w_1,\\dots, w_m)$ --- вектор весовых коэффициентов нейрона, $x=(x_1,\\dots, x_m)$ --- входные значения нейрона, а $\\tilde y$ --- желаемое выходное значение, соответствующее заданным входам. Тогда весовые коэффициенты сети следует изменять согласно следующей формуле:\n",
    "\\begin{equation}\\label{eq:delta}\n",
    "    w_j(t+1) = w_j(t) - \\alpha ( y - \\tilde y ) x_j,\n",
    "\\end{equation}\n",
    "где $t$ -- номер итерации, $\\alpha\\in(0,1)$ -- некоторый параметр (скорость обучения)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Что мешает обучать многослойный перцептрон используя выведенное ранее дельта-правило?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "У однослойной сети ошибки зависят только от одного слоя нейронов. Поэтому зная ответ сети и желаемый отклик из задачника можно подсчитать общую ошибку сети и рассчитать, как должны меняться весовые коэффициенты сети.\n",
    "\n",
    "У многослойной сети мы можем вычислить ошибки только для выходного слоя, но общая ошибка сети зависит от весовых коэффициентов всех слоев сети.\n",
    "\n",
    "**Мы не знаем, как нужно корректировать веса скрытых слоев сети, так как не знаем, насколько сильную ошибку вносят внутренние слои в общую ошибку.**\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Алгоритм обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Общая идея\n",
    "Обучение будет производиться также по схеме обучения с учителем, когда процедуре обучения передается два параметра: входные значения сети $(x_1, x_2, \\dots, x_m)$ и ожидаемые выходы $(\\tilde{y_1}, \\tilde{y_2}, \\dots, \\tilde{y_n})$, соответствующие заданным входам. \n",
    "\n",
    "Далее рассчитываются реальные выходные значения $(y_1, y_2, \\dots, y_n)$, которые получаются, если на входы сети подать заданные $(x_1, x_2, \\dots, x_m)$. В результате можно рассчитать общую ошибку работы сети для данного примера:\n",
    "\\begin{equation}\n",
    "E=\\frac12 \\sum_{i=1}^n (\\tilde{y_i}-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "Для того, чтобы уменьшить ошибку $E$, можно воспользоваться градиентными методами оптимизации.\n",
    "\n",
    "*(пока ничего нового не появилось)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Расчет ошибок\n",
    "Рассмотрим пример сети с одним скрытым слоем:\n",
    "\n",
    "<img src=\"img/mlp/net_mkn.png\" height=\"30%\">\n",
    "\n",
    "Рассчитаем выходные значения нейронов скрытого слоя $y^1_i$:\n",
    "\\begin{equation}\n",
    "    y^{(1)}_j = f(\\sum_{p=0}^m w_{ip}^0 x_p)\n",
    "\\end{equation}\n",
    "где $w_{jp}^0$ --- весовые коэффициенты нейрона $j$ скрытого слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/mlp/net_mkn.png\" height=\"30%\">\n",
    "\n",
    "Пусть уже рассчитаны выходные значения нейронов скрытого слоя $y^{(1)}_i$. Тогда выходной сигнал $j$-го нейрона выходного слоя рассчитывается по формуле:\n",
    "\\begin{equation}\n",
    "    y^{(2)}_j = f(\\sum_{i=0}^K w_{ji}^1 y_i^{(1)})\n",
    "\\end{equation}\n",
    "где $w_{ji}^1$ --- весовые коэффициенты нейронов выходного слоя, $j=1,2,\\dots,N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/mlp/net_mkn.png\" height=\"30%\">\n",
    "\n",
    "\n",
    "Таким образом, выходное значение сети рассчитывается по формуле:\n",
    "\n",
    "\\begin{equation}\n",
    "    y^{(2)}_j = f(\\sum_{i=0}^K w_{ji}^1 y_i^{(1)}) = f(\\sum_{i=0}^K w_{ji}^1 \\cdot (f(\\sum_{p=0}^m w_{ip}^0 x_p)) )\n",
    "\\end{equation}\n",
    "\n",
    "Аналогично можно рассчитать выходное значение сети с двумя, тремя и т.д. скрытыми слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вывод\n",
    "\n",
    "Таким образом ошибка сети для конкретного примера из задачника может быть рассчитана по формуле:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    E = \\frac12 \\sum_{j=1}^N \\left ( y^{(2)}_j - \\tilde y_j \\right )^2 = \\\\\n",
    "    = \\frac12  \\sum_{j=1}^N \\left (f \\left (\\sum_{i=0}^K w_{ji}^1 \\cdot \\left(f(\\sum_{p=0}^m w_{ip}^0 x_p)\\right) \\right) - \\tilde y_j \\right ) ^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "В этой формуле фигурируют веса всех нейронов сети, а не только нейронов выходного слоя. Следовательно, формула дает возможность рассчитать вклад в общую ошибку каждого весового коэффициента в отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Коррекция весов\n",
    "\n",
    "\n",
    "Итак, при обучении будем корректировать веса нейронов так, чтобы ошибка сети уменьшалась.\n",
    "\\begin{equation}\n",
    "    E = \\sum_{j=1}^N \\left ( y^{(2)}_j - \\tilde y_j \\right )^2 \\to \\min\n",
    "\\end{equation}\n",
    "\n",
    "Для этого воспользуемся градиентными методами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения: <<взгляд сверху>>\n",
    "\n",
    "Выберем очередной пример из задачника.\tАлгоритм будем выполнять в два этапа:\n",
    "\n",
    "* Прямой проход, во время которого рассчитываются отклики каждого слоя сети, начиная с первого и заканчивая последним, выходным, слоем.\n",
    "* Обратный проход, во время которого рассчитывается ошибка для каждого слоя сети, начиная с последнего (выходного) слоя и заканчивая первым слоем сети.\n",
    "\n",
    "После расчета ошибок производится коррекция весов, для того, чтобы уменьшить величину ошибки $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 0. Инициализация\n",
    "    \n",
    "* Составляем задачник.\n",
    "* Выбираем архитектуру сети (число слоев, число нейронов в слоях, функцию активации нейронов).\n",
    "* Генерируем синаптические веса и пороговые значения нейронов с помощью датчика случайных чисел со средним 0.\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 1. Предъявление примеров обучения (прямой проход).\n",
    "\n",
    "Пусть пример представлен парой векторов $(x_1, x_2, \\dots, x_m)$ --- входные значения сети и $(\\tilde{y_1}, \\tilde{y_2}, \\dots, \\tilde{y_n})$ --- ожидаемые выходы.\n",
    "\n",
    "1. Вычисляем потенциалы нейронов и функциональные сигналы:\n",
    "\\begin{equation}\\label{potenc}\n",
    "u_j^q = \\sum_i w_{ji}^q y_i^{q-1}; \\qquad y^q_j=f_j(u_j^q)\n",
    "\\end{equation}\n",
    "где $y_i^{q-1}$ -- выходной сигнал нейрона $i$, расположенного в предыдущем слое, $w_{ji}^q$ -- вес связи нейрона $j$ слоя $q$ с нейроном $i$ слоя $q-1$ (для $i=0$ считаем, что $y_0^{l-1}=1$, и $w_{j0}^q=b^q_j$ -- порог);  $f_j$ -- функция активации нейрона $j$. Здесь для удобства записи принято, что если нейрон находится в первом скрытом слое сети (т.е $q=1$), то считаем, что $y^0_j=x_j$.\n",
    "2. Вычисляем сигнал ошибки сети:\n",
    "\\begin{equation}\n",
    "    e_j=\\tilde{y}_j - y_j^Q\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 2. Представление примеров обучения (обратный проход).\n",
    "    \n",
    "3. Вычисляем локальные градиенты узлов сети по формуле:\n",
    "\\begin{equation}\\label{grad_main}\n",
    "    \\delta_j^q= \\left\\{ \n",
    "        \\begin{array}{ll}\n",
    "         e_j^Q f'_j(u_j) & \\mbox{для нейрона $j$ выходного слоя  $Q$} \\\\ \n",
    "         f'_j(u_j^q)\\sum_k \\delta^{q+1}_k w_{kj}^{q+1} & \\mbox{для нейрона $j$ скрытого слоя $q$}\n",
    "        \\end{array}  \\right .\n",
    "\\end{equation}\n",
    "4. Корректируем веса:\n",
    "\\begin{equation}\\label{korr_w}\n",
    "w_{ji}^q(t+1) = w_{ji}^q(t) + \\alpha w_{ji}^q(t-1) + \\eta \\delta^q_j(t) y^{q-1}_i(t)\n",
    "\\end{equation}\n",
    "где $t$ --- номер итерации, $\\alpha \\in [0,1)$ и $\\eta \\in (0,1)$ --- параметры, влияющие на скорость градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 3. Итерации\n",
    "\n",
    "Последовательно выполняем прямой и обратный проходы, предъявляя сети все примеры обучения, пока не будет достигнут критерий останова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: Параметры скорости и момента\n",
    "    \n",
    "При коррекции весов используется параметр момента $\\alpha$  и скорость обучения $\\eta$. \n",
    "\\begin{equation*}\n",
    "    w_{ji}^q(t+1) = w_{ji}^q(t) + \\alpha w_{ji}^q(t-1) + \\eta \\delta^q_j(t) y^{q-1}_i(t)\n",
    "\\end{equation*}\n",
    "\n",
    "* Чем меньше параметр скорости $\\eta$ => тем меньше корректировка весов => тем более гладкой будет траектория изменения весов $w_{ji}^q$. Но это улучшение происходит за счет замедления обучения.\n",
    "* Если увеличить $\\eta$ для повышения скорости => большие изменения весов => возможно неустойчивое состояние системы.\n",
    "\n",
    "Вводится момент $\\alpha$ => ускорение обучения, если веса изменяются в одном направлении, и стабилизация, если веса сети менялись в разных направлениях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: Критерии останова\n",
    "\n",
    "Критериев может быть много и разных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Обучение останавливается при достижении такого состояния, когда сеть становится способна обобщать примеры, предъявленные ей в прошлом, на другие, незнакомые примеры.\n",
    "\n",
    "\n",
    "Для определения было ли достигнуто данное состояние используется процедура перекрестной проверки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Перекрестная проверка\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Явление переобучения\n",
    "<img src=\"img/mlp/trolernado.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Перекрестная проверка: классический вариант\n",
    "\n",
    "\n",
    "* Данные случайным образом разбивают на обучающее множество и тестовое множество. Обучающее множество, в свою очередь, разбивают на два подмножества: оценочное подмножество и проверочное (валидационное) подмножество.\n",
    "* Сеть обучают на оценочном подмножестве. \t\n",
    "Чтобы избежать переобучения, сеть постоянно проверяют, предъявляя ей незнакомые примеры из проверочного подмножества. (Обычно суммарная ошибка сначала падает, потом начинает возрастать).\n",
    "* Проверочное множество также участвовало в процедуре обучения и в результате может оказаться, что сеть была переобучена на проверочном множестве. Поэтому используют третье, тестовое множество, которое не участвовало в процедуре обучения. На этом множестве и проверяют качество работы сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Функции активации\n",
    "\n",
    "В формуле коррекции весов учавствуют производная функции активации:\n",
    "\n",
    "Вычисляем локальные градиенты узлов сети по формуле\n",
    "    \\begin{equation*}\n",
    "        \\delta_j^q= \\left\\{ \n",
    "            \\begin{array}{ll}\n",
    "             e_j^Q f'_j(u_j) & \\mbox{для нейрона $j$ выходного слоя  $Q$} \\\\ \n",
    "             f'_j(u_j^q)\\sum_k \\delta^{q+1}_k w_{kj}^{q+1} & \\mbox{для нейрона $j$ скрытого слоя $q$}\n",
    "            \\end{array}  \\right .\n",
    "    \\end{equation*}\n",
    "\n",
    "Корректируем веса с помощью локальных градиентов:\n",
    "\\begin{equation*}\n",
    "    w_{ji}^q(t+1) = w_{ji}^q(t) + \\alpha w_{ji}^q(t-1) + \\eta \\delta^q_j(t) y^{q-1}_i(t)\n",
    "\\end{equation*}\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Сигмоидальная функция активации\n",
    "Функция активации записывается следующим образом:\n",
    "\\begin{equation}\n",
    "y= f(u) = \\frac{1}{1+e^{-au}}\n",
    "\\end{equation}\n",
    "где $a>0$.\n",
    "Производная этой функции может быть представлена в виде:\n",
    "\\begin{equation}\n",
    "f'(u) = a y (1-y).\n",
    "\\end{equation}\n",
    "\n",
    "Тогда градиенты в узлах сети рассчитываются по формуле:\n",
    "\\begin{equation}\n",
    "\\delta_j^q= \\left\\{ \n",
    "\t\\begin{array}{ll}\n",
    "\t a (\\tilde y_j - y_j^Q) y_j^Q (1- y_j^Q) & \\mbox{для нейрона $j$ выходного слоя $Q$} \\\\ \n",
    "\t a y^q_j (1- y^q_j) \\sum_k \\delta^{q+1}_k w_{kj}^{q+1} & \\mbox{для нейрона $j$ скрытого слоя $q$}\n",
    "\t\\end{array}  \\right .\n",
    "\\end{equation}\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Гиперболический тангенс\n",
    "Функция активации записывается следующим образом:\n",
    "\\begin{equation}\n",
    "y= f(u) = a th(bu)\n",
    "\\end{equation}\n",
    "где $a>0$ и $b>0$.\n",
    "Производная этой функции может быть представлена в виде:\n",
    "\\begin{equation}\n",
    "f'(u) = \\frac{b}{a} (a-y)(a+y).\n",
    "\\end{equation}\n",
    "Тогда градиенты в узлах сети рассчитываются по формуле:\n",
    "\\begin{equation}\n",
    "\\delta_j^q= \\left\\{ \n",
    "\t\\begin{array}{ll}\n",
    "\t \\frac{b}{a} (\\tilde y_j - y_j^Q) (a-y_j^Q) (a+ y_j^Q) & \\mbox{для нейрона $j$ выходного слоя $Q$} \\\\ \n",
    "\t \\frac{b}{a} (a-y_j^q) (a+ y_j^q) \\sum_k \\delta^{q+1}_k w_{kj}^{q+1} & \\mbox{для нейрона $j$ скрытого слоя $q$}\n",
    "\t\\end{array}  \\right .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Программная реализация\n",
    "\n",
    "[В основе - реализация Nicolas P. Rougier (BSD License)](https://github.com/rougier/neural-networks/blob/master/mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -----------------------------------------------------------------------------\n",
    "# Multi-layer perceptron\n",
    "# Copyright (C) 2011  Nicolas P. Rougier\n",
    "#\n",
    "# Distributed under the terms of the BSD License.\n",
    "# -----------------------------------------------------------------------------\n",
    "# This is an implementation of the multi-layer perceptron with retropropagation\n",
    "# learning.\n",
    "# -----------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    ''' Sigmoid like function using tanh '''\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dsigmoid(x):\n",
    "    ''' Derivative of sigmoid above '''\n",
    "    return (1.0-x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, *args):\n",
    "        ''' Инициализация перцептрона '''\n",
    "\n",
    "        self.shape = args\n",
    "        n = len(args)\n",
    "\n",
    "        # Создаем массивы для хранения выходов из нейронов\n",
    "        self.layers = []\n",
    "        # Входной слой (+1 для порога)\n",
    "        self.layers.append(np.ones(self.shape[0]+1))\n",
    "        # Скрытые слои + выходной слой\n",
    "        for i in range(1,n):\n",
    "            self.layers.append(np.ones(self.shape[i]))\n",
    "\n",
    "        # Матрицы весов\n",
    "        self.weights = []\n",
    "        for i in range(n-1):\n",
    "            self.weights.append(np.zeros((self.layers[i].size,\n",
    "                                         self.layers[i+1].size)))\n",
    "\n",
    "        # dw будет хранить величину последних изменений весов (для момента)\n",
    "        self.dw = [0,]*len(self.weights)\n",
    "\n",
    "        # Reset weights\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        ''' Reset weights '''\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.random.random((self.layers[i].size,self.layers[i+1].size))\n",
    "            self.weights[i][...] = (2*Z-1)*0.25\n",
    "\n",
    "    def propagate_forward(self, data):\n",
    "        '''Прямой проход. '''\n",
    "\n",
    "        # Set input layer\n",
    "        self.layers[0][0:-1] = data\n",
    "\n",
    "        # Прямой проход от слоя 0 к слою n-1 (и прогон через функции активации)\n",
    "        for i in range(1,len(self.shape)):\n",
    "            # Propagate activity\n",
    "            self.layers[i][...] = sigmoid(np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "\n",
    "        # Return output\n",
    "        return self.layers[-1]\n",
    "\n",
    "\n",
    "    def propagate_backward(self, target, lrate=0.1, momentum=0.1):\n",
    "        '''Обратный проход'''\n",
    "\n",
    "        deltas = []\n",
    "\n",
    "        # Ошибки на выходном слое\n",
    "        error = target - self.layers[-1]\n",
    "        delta = error*dsigmoid(self.layers[-1])\n",
    "        deltas.append(delta)\n",
    "\n",
    "        # Ошибки скрытых слоев\n",
    "        for i in range(len(self.shape)-2,0,-1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * dsigmoid(self.layers[i])\n",
    "            deltas.insert(0, delta)\n",
    "            \n",
    "        # Обновление весов\n",
    "        for i in range(len(self.weights)):\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            delta = np.atleast_2d(deltas[i])\n",
    "            dw = np.dot(layer.T,delta)\n",
    "            self.weights[i] += lrate*dw + momentum*self.dw[i]\n",
    "            self.dw[i] = dw\n",
    "\n",
    "        # Вернуть общую ошибку сети\n",
    "        return (error**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "print \"Learning the sin function\"\n",
    "network = MLP(1, 15, 1)\n",
    "samples = np.zeros(500, dtype=[('x',  float, 1), ('y', float, 1)])\n",
    "samples['x'] = np.linspace(0,1, 500)\n",
    "samples['y'] = np.sin(samples['x']*np.pi)\n",
    "\n",
    "for i in range(10000):\n",
    "    n = np.random.randint(samples.size)\n",
    "    network.propagate_forward(samples['x'][n])\n",
    "    network.propagate_backward(samples['y'][n], lrate=.1, momentum=0.1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "# Draw real function\n",
    "x,y = samples['x'],samples['y']\n",
    "plt.plot(x, y, color='b', lw=1)\n",
    "# Draw network approximated function\n",
    "for i in range(samples.shape[0]):\n",
    "    y[i] = network.propagate_forward(x[i])\n",
    "plt.plot(x,y, color='r', lw=3)\n",
    "plt.axis([-0.1, 1.1, -0.1, 1.1])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
