{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Элеметы теории статистического обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Задача:\n",
    "\n",
    "Имется две пары игральных костей. Одна стандартная (на гранях числа от 1 до 6), а вторая \"подправленная\", где на каждой грани записано число на два больше (т.е. от 3 до 9). Я случайно с равной вероятностью выбираю или первую пару костей или вторую, а затем бросаю их. Вам сообщаю число выпавших очков, ваша задача - угадать, какую пару костей я подбросил стандартную или \"подправленную\". Если угадываете - получаете 10 рублей, не угадываете - вы платите мне 10 рублей :))\n",
    "\n",
    "Допустим, я сообщаю, что выпало 8 очков. На что вы поставите?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ответ\n",
    "<img src=\"img/lecBayes/DiceProbas.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    " 2   1,1                          -\n",
    " 3   1,2 2,1                      - \n",
    " 4   1,3 2,2 3,1                  -\n",
    " 5   1,4 2,3 3,2 4,1              -\n",
    " 6   1,5 2,4 3,3 4,2 5,1          3,3\n",
    " 7   1,6 2,5 3,4 4,3 5,2 1,6      3,4 4,3\n",
    " 8   2,6 3,5 4,4 5,4 6,2          3,5 4,4 5,4\n",
    " 9   3,6 4,5 5,4 6,3              3,6 4,5 5,4 6,3\n",
    "10   4,6 5,5 6,4                  3,7 4,6 5,5 6,4 7,3\n",
    "11   5,6 6,5                      3,8 4,7 5,6 6,5 7,4 8,3\n",
    "12   6,6                          4,8 5,7 6,6 7,5 8,4\n",
    "13   -                            5,8 6,7 7,6 8,5 \n",
    "14   -                            6,8 7,7 8,6\n",
    "15   -                            7,8 8,7\n",
    "16   -                            8,8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/lecBayes/DiceProbas1.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Формализация\n",
    "\n",
    "* Имеется несколько типов (классов) объектов (ситуаций): $1, 2, \\dots, K$.\n",
    "* Каждый объект (ситуация) описывается некоторым набором параметров, которые формируют вектор признаков: $X = (x_1, x_2, \\dots, x_p) \\in \\mathscr{X}$. (Обычно $\\mathscr{X}$ -- подмножество $\\mathbb{R}^p$).\n",
    "* Пропорция каждого класса $k$ в общей \"популяции\" объектов: $\\pi_k$ (может быть известна заранее).\n",
    "* Объекты каждого класса $k$ распределены в соответствии с плотностью распределения (вероятностью) $p_k(x)$, которая является функцией от $x$.\n",
    "\n",
    "*Задача:* классифицировать предъявляемый классификатору объект. Т.е. глядя на вектор признаков объекта $X=x$ вернуть одно из $K+2$ возможных решений: $1, 2, \\dots, K, \\mathscr{D}, \\mathscr{O}$ (где $\\mathscr{D}$ - отказ от классификации, $\\mathscr{O}$ - выброс, т.е. объект точно не принадлежит ни одному из $K$ классов).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Простейший случай: пропорции классов $\\pi_k$ известны, выбросы не учитываются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Предположим, мы построили два классификатора. Как узнать, который из них лучше?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Нам понадобятся: \n",
    "* вероятность ошибки классификатора для каждого класса ($\\hat{c}(X)$ -- какой ответ выдает классификатор для объекта с вектором признаков $X$): \n",
    "\\begin{equation*}\n",
    "    p_{er}(k) = P\\{\\hat{c}(X) \\neq k, \\quad \\hat{c}(X)\\in \\{1, 2, \\dots, K\\} \\quad |\\quad C=k \\}\n",
    "\\end{equation*}\n",
    "* вероятность отказа от классификации:\n",
    "\\begin{equation*}\n",
    "    p_{d}(k) = P\\{\\hat{c}(X) = \\mathscr{D} \\quad | \\quad C=k \\}\n",
    "\\end{equation*}\n",
    "* величина проигрыша при неверно выбранном ответе (функция потерь)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Функция потерь (loss function)\n",
    "\n",
    "Пусть $L(k, l)$ -- величина потерь при ответе \"объект принадлежит классу $l$\", если на самом деле объект принадлежит классу $k$. (Логично: $L(k,k) = 0$).\n",
    "\\begin{equation*}\n",
    "    L(k,l)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            0, & \\mbox{если } l= k \\mbox{ (корректный ответ)}\\\\\n",
    "            1, & \\mbox{если } l\\neq k \\mbox{ и } l\\in \\{1,2,\\dots,K\\} \\\\\n",
    "            d, & \\mbox{если } l=\\mathscr{D} \\mbox{ (в затруднении)}.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Функция риска\n",
    "\n",
    "Ожидаемая величина потерь для классификатора $\\hat{c}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    R(\\hat{c}, k) = E[L(k, \\hat{c}(X)) \\ | \\  C=k] = \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "    \\sum_{l=1}^K L(k, l) P\\{\\hat{c}(X) =l \\ | \\ C=k\\} + L(k, \\mathscr{D}) P\\{\\hat{c}(X)=\\mathscr{D} \\ | \\ C=k\\}) =\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "   = p_{er}(k) + d p_d(k).\n",
    "\\end{equation*}\n",
    "\n",
    "Общий риск когда ожидаемый класс $C$ и вектор $X$ случайны:\n",
    "\n",
    "\\begin{equation*}\n",
    "   R(\\hat{c}) = E R(\\hat{c}, C) = \\sum_{k=1}^k \\pi_k p_{er}(k) + d \\sum_{k=1}^k \\pi_k p_d(k).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(Вспоминаем гистограммы)\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "   p(k \\ | \\ x) = P\\{C=k \\ | \\ X=x\\} = \\frac{\\pi_k p_k(x)}{\\sum_{l=1}^K \\pi_j p_l(x)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Правило классификации\n",
    "Для минимизации риска \n",
    "\\begin{equation*}\n",
    "    L(k,l)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            0, & \\mbox{если } l= k \\mbox{ (корректный ответ)}\\\\\n",
    "            1, & \\mbox{если } l\\neq k \\mbox{ и } l\\in \\{1,2,\\dots,K\\} \\\\\n",
    "            d, & \\mbox{если } l=\\mathscr{D} \\mbox{ (в затруднении)}.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}\n",
    "\n",
    "нужно действовать по правилу:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "    c(x)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            k, & \\mbox{если } p(k|x) = \\max_{l \\leq K} \\mbox{ и это больше } 1-d\\\\\n",
    "            \\mathscr{D}, & \\mbox{если каждый} p(k|x) \\leq 1-d.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "Для минимизации общего риска нужно действовать по правилу:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "    c(x)  = \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            k, & \\mbox{если это дает минимум }  min_{l\\leq K} \\sum L(j, l) p(j | x) < d\\\\\n",
    "            \\mathscr{D}, & \\mbox{иначе}.\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <<Проклятие размерности>>\n",
    "### Эффект Хьюза (Hughes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Есть (квантованное) пространство признаков, состоящее из $n$ элементов.\n",
    "* Строим все возможные классификаторы в этом пространстве и вычисляем среднюю точность по ним.\n",
    "\n",
    "<img src=\"img/lecBayes/Hughes0.png\" height=\"30%\">\n",
    "Hughes G. On the mean accuracy of statistical pattern recognizers //IEEE transactions on information theory. – 1968\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Бесконечная обучающая выборка\n",
    "Точность классификатора напрямую зависит от $n$: чем больше признаков, тем точнее результат (но существует предел).\n",
    "<img src=\"img/lecBayes/Hughes1.png\" height=\"30%\">\n",
    "Hughes G. On the mean accuracy of statistical pattern recognizers //IEEE transactions on information theory. – 1968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Конечная обучающая выборка\n",
    "Зависимость точности от размера обучающей выборки (для случая равновероятных классов).\n",
    "<img src=\"img/lecBayes/Hughes2.png\" height=\"30%\">\n",
    "Hughes G. On the mean accuracy of statistical pattern recognizers //IEEE transactions on information theory. – 1968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Почему при конечной выборке сначала точность классификаторов увеличивается, а затем после некоторого предела падает?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Две противоположные тенденции:\n",
    " * с увеличением размерности разделимость классов также увеличивается (может увеличиться);\n",
    " * чем больше размерность, тем больше параметров классификатора нужно оценить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Отбор признаков\n",
    "\n",
    "Как оценить качество набора признаков?\n",
    "\n",
    "<img src=\"img/lecBayes/DiceProbas.png\" height=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Дивергенция - мера несходства между двумя классами\n",
    "<img src=\"img/lecBayes/Divergence.png\" height=\"30%\">\n",
    "\n",
    "Отношение плотностей и логарифм отношения:\n",
    "$$\n",
    "L_{ij} = \\frac{p(X|C=i)}{p(X|C=j)}; \\qquad L'_{ij} = \\ln [p(X|C=i)] - \\ln[p(X|C=j)]\n",
    "$$\n",
    "\n",
    "Дивергенция:\n",
    "\n",
    "$$\n",
    "D_{ij} = E\\big (L'_{ij}| C=i \\big) + E\\big ( L'_{ji} | C=j\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(где $E\\big (L'_{ij}| C=1 \\big) = \\int_x p(x|C=i) \\ln \\frac{p(x|C=i)}{p(x|C=j)} dx $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VC-размерность\n",
    "\n",
    "* Есть два класса.\n",
    "* Мы хотим построить классификатор, которые разделяет выбранные классы.\n",
    "\n",
    "Вопрос <<Сколько параметров должно быть у классификатора, чтобы различать классы>> тесно связан с понятием VC-измерения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Дихотомия\n",
    "\n",
    "Задача разделения двух классов $C_0$ и $C_1$ в пространстве $\\mathscr{X} \\subset \\mathbb{R}^m$. Обозначим через $\\mathbb{F}$ множество дихотомий, реализованных обучающей машиной:\n",
    "\n",
    "$$\n",
    "\\mathbb{F} = \\{F(x, w): w\\in\\mathbb{W}, F: \\mathscr{X}\\times\\mathbb{W} \\to \\{0, 1\\} \\}\n",
    "$$\n",
    "\n",
    "Пусть $\\mathbb{L}$ -- множество из $N$ точек $m$-мерного пространства $\\mathscr{X}$\n",
    "$$\n",
    "\\mathbb{L} = \\{x_i \\in \\mathscr{X}; i=1, 2, \\dots, N\\}\n",
    "$$\n",
    "\n",
    "Дихотомия, реализованная обучаемой машиной разбивает $\\mathbb{L}$ на два непересекающихся подмножества $\\mathbb{L}_0$ и $\\mathbb{L}_1$:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\tF(x, w) = \t\\left\\lbrace \n",
    "\t\t\t\t\\begin{array}{rl}\n",
    "\t\t\t\t\t0, & \\mbox{ для } x\\in \\mathbb{L}_0\\\\\n",
    "\t\t\t\t\t1, & \\mbox{ для } x\\in \\mathbb{L}_1\n",
    "\t\t\t\t\\end{array}  \\right. .\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"img/lecBayes/Dichotomy.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### VC-размерность\n",
    "\n",
    "Определение:\n",
    "\n",
    "**VC-размерностью называется мощность наибольшего множества $\\mathbb{L}$, разбиением которого является $\\mathbb{F}$**\n",
    "\n",
    "Другими словами VC-размерностью называют самое большое значение $N$ для которого машина может реализовать все $2^N$ разбиений.\n",
    "\n",
    "<img src=\"img/lecBayes/Dichotomy1.png\" height=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Применение к нейросетям.\n",
    "\n",
    "* Пусть решающее правило задается формулой \n",
    "$$F: y = \\phi (w\\times x + b)$$ где $w$ -- $m$-мерный вектор весов, $b$ -- порог, функция активации пороговая \n",
    "$$\\phi (u)= \\left\\lbrace \n",
    "\t\t\t\t\\begin{array}{rl}\n",
    "\t\t\t\t\t1, & \\mbox{если $u \\geq 0$}\\\\\n",
    "\t\t\t\t\t-1, & \\mbox{если u<0}\n",
    "\t\t\t\t\\end{array}  \\right.$$\n",
    "Тогда VC-размерность правила равна $m+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Пусть NN -- произвольная нейросеть прямого распространения, состоящая из нейронов с пороговой функцией активации. Тогда VC-размерность нейросети составляет $O(W \\log W)$, где $W$ - общее количество свободных параметров сети. (Baum E. B., Haussler D. What size net gives valid generalization? //Advances in neural information processing systems. – 1989.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Пусть NN -- произвольная нейросеть прямого распространения, состоящая из нейронов с сигмоидальной функцией активации\n",
    "\\begin{equation}\n",
    "    f(u) = \\frac{1}{1+e^{-au}},\n",
    "\\end{equation}\t\n",
    "Тогда VC-размерность нейросети составляет $O(W^2)$, где $W$ - общее количество свободных параметров сети. (Koiran P., Sontag E. D. Neural networks with quadratic VC dimension //Advances in neural information processing systems. – 1996)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
