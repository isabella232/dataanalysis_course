{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Нейронные сети. Введение\n",
    "\n",
    "* Колесов Дмитрий Александрович <dmitry.kolesov@nextgis.ru>\n",
    "\n",
    "[https://github.com/nextgis/dataanalysis_course](https://github.com/nextgis/dataanalysis_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Литература и источники\n",
    "\n",
    "## Книги\n",
    "1. [Любая книга, которую вы найдете](https://scholar.google.ru/scholar?hl=ru&as_sdt=0%2C5&q=%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5+%D1%81%D0%B5%D1%82%D0%B8&btnG=).\n",
    "2. [Хайкин С. Нейронные сети: полный курс, 2-е издание. – Издательский дом Вильямс, 2008.](https://www.google.com/books?hl=ru&lr=&id=LPMr0iA0muwC&oi=fnd&pg=PA22&ots=4d1Hh4_NA7&sig=WhhjlewVu5Jc2px6kNndeLMXB68)\n",
    "3. [Николенко С. И., Кадурин А. А., Архангельская Е. О. Глубокое обучение СПб: Питер, 2019.](https://www.piter.com/product/glubokoe-obuchenie)\n",
    "6. [Гудфеллоу Я., Иошуа Б., Курвилль А. Глубокое обучение. – Litres, 2017](https://www.google.com/books?hl=ru&lr=&id=741EDwAAQBAJ&oi=fnd&pg=PA14&dq=related:1llkUhAywd8J:scholar.google.com/&ots=3bcjOiPuxy&sig=7K-nf3JD8Hjzo42xCOkoQKMgkAI).\n",
    " (Доступна онлайн [Исходная (английская) версия](http://www.deeplearningbook.org/).)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Курсы/Видеолекции\n",
    "1. Курс \"Convolutional Neural Networks for Visual Recognition\" Стендфордского университета.\n",
    " * [Страница курса](http://cs231n.stanford.edu/).\n",
    " * [Видеолекции на Youtube](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC).\n",
    " \n",
    "2. Курс \"Машинное обучение\" Школы анализа данных, Яндекс.\n",
    " * [Видеолекции](https://yandexdataschool.ru/edu-process/courses/machine-learning).\n",
    " \n",
    "3. Курс \"Learning from data\" Калифорнийского технологического института\n",
    " * [Страница курса](https://work.caltech.edu/telecourse.html).\n",
    " * [Видеолекции на Youtube](https://www.youtube.com/playlist?list=PLD63A284B7615313A).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Практика\n",
    "\n",
    "### Дистанционное зондирование\n",
    " * [Google EarthEngine](http://earthengine.google.com)\n",
    " * [Открытая коллекция аэрофотосьемки](https://openaerialmap.org/)\n",
    " * [Сайт космической программы Landsat](http://landsat.gsfc.nasa.gov)\n",
    " * [Сайт космической программы MODIS](http://modis.gsfc.nasa.gov)\n",
    " * [Сайт геологической службы США](https://earthexplorer.usgs.gov/) \n",
    "\n",
    "### Соревнования\n",
    " * [Kaggle](https://www.kaggle.com/).\n",
    "\n",
    "### Наборы данных\n",
    " * [Kaggle](https://www.kaggle.com/).\n",
    " * [UCI Machine Learning Repository](http://mlr.cs.umass.edu/ml/datasets.html).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Определения\n",
    " * [Биологическая нейронная сеть](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C) — совокупность нейронов головного и спинного мозга центральной нервной системы (ЦНС) и ганглия периферической нервной системы, которые связаны или функционально объединены в нервной системе, выполняют специфические физиологические функции.\n",
    " * [Искусственная нейронная сеть](https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C) — математическая модель, а также её программное или аппаратное воплощение, построенная по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Предпосылки\n",
    "\n",
    "Порассуждаем о том, зачем люди вообще занимаются исследованием нейронных сетей и что они дают.\n",
    "\n",
    "Все знают, как устроен <<обычный>> фон-Неймановский компьютер, каковы его сильные строны и что он умеет делать. Что *не умеет делать* обычный компьютер? Или что он делает очень плохо?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Биологические предпосылки: пример 1\n",
    "\n",
    "Распознавание образов:\n",
    "\n",
    "<img src=\"img/cat_dog.jpg\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ImageRecErrRate.png\" height=\"10%\">\n",
    "\n",
    "Снижение ошибки распознавания объектов на данных ImageNet Large Scale Visual Recognition Challenge с течением времени (Гудфеллоу Я. и др. \"Глубокое обучение\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Биологические предпосылки: пример 2\n",
    "\n",
    "Сонар летучей мыши.\n",
    "\n",
    "* Расстояние до нужного объекта\n",
    "* Относительная скорость объекта\n",
    "* Размеры его отдельных элементов\n",
    "* Азимут и высота движения объекта\n",
    "\n",
    "И все это в полете.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Технические предпосылки\n",
    "\n",
    "Архитектура [МКМД (MIMD)](https://ru.wikipedia.org/wiki/MIMD) заходит в тупик? (Галушкин А.И. \"Теория нейронных сетей\".)\n",
    "\n",
    "![Производительность на один вычислительный узел (ядро процессора).](img/MIMD.png)\n",
    "\n",
    "* Потери при межузловых обменах при увеличении числа узлов.\n",
    "* Увеличение стоимости узла при увеличении его производительности.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Развитие нейросетей \n",
    "\n",
    "(Гудфеллоу Я. и др. \"Глубокое обучение\").\n",
    "\n",
    "![Размеры нейросетей](img/NnetSizes.png)\n",
    "\n",
    "Рост размеров сетей со временем.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ссылки|Ссылки\n",
    "-|-\n",
    "1. Perceptron (Rosenblatt, 1958, 1962). |11. GPU-accelerated convolutional network (Chellapilla et al., 2006). \n",
    "2. Adaptive linear element (Widrow and Hoff, 1960). |12. Deep Boltzmann machine (Salakhutdinov and Hinton, 2009a).  \n",
    "3. Neocognitron (Fukushima, 1980). | 13. GPU-accelerated deep belief network (Raina et al., 2009).\n",
    "4. Early back-propagation network (Rumelhart et al., 1986b).|14. Unsupervised convolutional network (Jarrett et al., 2009). \n",
    "5. Recurrent neural network for speech recognition (Robinson and Fallside, 1991).| 15. GPU-accelerated multilayer perceptron (Ciresan et al., 2010). \n",
    "6. Multilayer perceptron for speech recognition (Bengio et al., 1991). |16. OMP-1 network (Coates and Ng, 2011). \n",
    "7. Mean field sigmoid belief network (Saul et al., 1996). | 17. Distributed autoencoder (Le et al., 2012). \n",
    "8. LeNet-5 (LeCun et al., 1998b). |18. Multi-GPU convolutional network (Krizhevsky et al., 2012). \n",
    "9. Echo state network (Jaeger and Haas, 2004). |19. COTS HPC unsupervised convolutional network (Coates et al., 2013).\n",
    "10. Deep belief network (Hinton et al., 2006). | 20. GoogLeNet (Szegedy et al., 2014a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Примеры\n",
    "\n",
    " * [Самолет-разведчик LoFLYTE](http://www.nasa.gov/centers/langley/news/factsheets/LoFlyte.html) использовал нейронные сети, позволяющие автопилоту обучаться, копируя приемы пилотирования летчика.\n",
    " * [Игровая индустрия](https://ru.wikipedia.org/wiki/AlphaGo).\n",
    " * [Поисковые системы](https://yandex.ru/blog/company/algoritm-palekh-kak-neyronnye-seti-pomogayut-poisku-yandeksa).\n",
    " * [Автоматические переводчики](https://yandex.ru/blog/company/kak-pobedit-mornikov-yandeks-zapustil-gibridnuyu-sistemu-perevoda).\n",
    " * [\"Котики\"](http://www.image-net.org/challenges/LSVRC/).\n",
    " * И множество других областей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Модель нейрона"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вопросы, возникающие при изучении мозга\n",
    "\n",
    "* Как работает нервная клетка - биологический нейрон?\n",
    "(Необходимо иметь математическую модель, адекватно описывающую информационные процессы в нейроне. Какие свойства нейрона важны при моделировании, а какие - нет?)\n",
    "* Как передается информация через соединения между\n",
    "нейронами - синапсы? Как меняется проводимость синапса в\n",
    "зависимости от проходящих по нему сигналов?\n",
    "* По каким законам нейроны связаны друг с другом в\n",
    "сеть? Откуда нервная клетка <<знает>>, с какими соседями должно быть установлено соединение?\n",
    "* Как биологические нейронные сети обучаются решать задачи?\n",
    "Как выбираются параметры сети, чтобы давать правильные выходные сигналы? Какой выходной сигнал считается  <<правильным>>, а какой - ошибочным?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Пирамидальный нейрон\n",
    "\n",
    "* Тело клетки содержит ядро, митохондрии и др. органелы,  поддерживающие жизнедеятельность клетки.\n",
    "* Дендриты - входные волокна, собирают информацию от других нейронов.\n",
    "* Аксон - длинное выходное нервное волокно клетки. Обеспечивает проведение импульса и передачу его в другие нейроны.\n",
    "* Синапс - место контакта нервных волокон, передает импульс от нейрона к нейрону.\n",
    "![Пирамидальный нейрон](img/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Простейшая модель нейрона\n",
    "\n",
    "![Пирамидальный нейрон](img/neuron_mod5.png)\n",
    "\n",
    "\n",
    "$$\n",
    "u = \\sum_{i=0}^n w_i x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = f(u) = f\\left(\\sum_{i=0}^n w_i x_i\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Пример нейронной сети: многослойный перцептрон\n",
    "\n",
    "\n",
    "![Многослойный перцептрон](img/net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Виды функций активаций\n",
    "\n",
    "1. Пороговая\n",
    "\\begin{equation}\n",
    "\t\t\ty = f(u)= \\left\\lbrace \n",
    "\t\t\t\t\\begin{array}{rl}\n",
    "\t\t\t\t\t1, & \\mbox{если $u \\geq 0$}\\\\\n",
    "\t\t\t\t\t-1, & \\mbox{если u<0}\n",
    "\t\t\t\t\\end{array}  \\right. .\n",
    "\\end{equation}\n",
    "\n",
    "2. Линейная ограниченная.\n",
    "\\begin{equation}\n",
    "y = f(u)= \\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "        1, & \\mbox{если $u > 1$}\\\\\n",
    "        u, & \\mbox{если $-1 \\leq u \\leq 1$} \\\\\n",
    "        -1, & \\mbox{если u<-1}\n",
    "    \\end{array}  \\right. .\n",
    "\\end{equation}\t\n",
    "\n",
    "3. Выпрямитель (ReLU).\n",
    "\\begin{equation}\n",
    "y = f(u)= \\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "        0, & \\mbox{если $u < 0$}\\\\\n",
    "        u, & \\mbox{если $u \\geq 0$}\n",
    "    \\end{array}   \\right. \n",
    "\\end{equation}\t\n",
    "\n",
    "4. Логистическая (сигмоидальная/сигмоидная)\n",
    "\\begin{equation}\n",
    "    y = f(u) = \\frac{1}{1+e^{-au}},\n",
    "\\end{equation}\t\n",
    "\n",
    "5. Гиперболический тангенс.\n",
    "\\begin{equation}\n",
    "    y = f(u) = th(b u)= \\frac{e^{bu}- e^{-bu}}{e^{bu}+e^{-bu}},\n",
    "\\end{equation}\t\t\n",
    "\n",
    "6. Линейная\n",
    "\\begin{equation}\n",
    "    y = f(u) = k u\n",
    "\\end{equation}\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Примеры\n",
    "\n",
    "Дан нейрон с линейным сумматором и пороговой функцией активации. Пусть на входы нейрона поданы сигналы $x_1=0.1$, $x_2=0.1$, $x_3=0.5$, $x_4=0.9$. Расчитать выходное значение нейрона.\n",
    "![Пример 1](img/primer1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "(Ответ: \n",
    "\\begin{equation*}\n",
    "    u = b+  \\sum_{i=1}^{4} x_i w_i = 0.2 + 0.1\\times 0.5+0.1\\times 0.1-0.5\\times 0.3+0.9\\times 0.4 = 0.47; \\qquad y = f(u) = 1\n",
    "\\end{equation*}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Пример 2\n",
    "\n",
    "![Пример 1](img/robot.png)\n",
    "\n",
    "Требуется разработать мозги робота, который ищет себе пищу (пища = свет). У робота есть два светодатчика: левый и правый. Робот должен идти в ту сторону, где света больше.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Сколько синаптических связей должен иметь нейрон?\n",
    "* Какую функцию активации выбрать?\n",
    "* Как интерпретировать выходное значение нейрона?\n",
    "* Чему должно быть равно пороговое значение?\n",
    "* Чему должны быть равны синаптические веса?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Два входа: $x_1$ -- левый датчик, $x_2$ -- правый датчик.\n",
    "* Пороговая функция активации.\n",
    "* $y=1$: поворот налево; $y=-1$ : поворот направо.\n",
    "* $b=0$.\n",
    "* $w_1=1; \\qquad w_2=-1$.\n",
    "\n",
    "*Что получится, если выбрать порог $b \\ne 0$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Однослойный перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Рассматриваем однослойную нейронную сеть.\n",
    "\n",
    "<img src=\"img/oneLayerNet.png\" height=\"30%\">\n",
    "Сеть состоит из $n$ нейронов, каждый нейрон имеет $m+1$ вход.\n",
    "У нейрона номер $i$ весовые коэффициенты $w_{0i}, w_{1i}, \\dots w_{mi}$ ($w_{0i}$ -- вес порогового входа, на схеме не показан).\n",
    "\n",
    "Видим, что нейрон номер $i$ никак не зависит от нейрона номер $j$ (нейроны могут обучаться независимо друг от друга)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Правило обучения Розенблатта\n",
    "\n",
    "Рассматриваем перцепртрон: однослойную нейронную сеть с порговой функцией активации. Без потери общности обучаем отдельный нейрон $i$.\n",
    "\n",
    "\\begin{equation}\n",
    "u = b+\\sum_{i=1}^n x_i w_i ;\\quad y = f(u)= \\left\\lbrace \n",
    "\t\t\t\t\\begin{array}{rl}\n",
    "\t\t\t\t\t1, & \\mbox{если $u \\geq 0$}\\\\\n",
    "\t\t\t\t\t-1, & \\mbox{если u<0}\n",
    "\t\t\t\t\\end{array}  \\right. . \n",
    "\\end{equation}\n",
    "\n",
    "Схема \"обучение с учителем\": есть задачник, содержащий входные значения и желаемый выход $\\tilde y \\in \\{-1, 1\\}$.\n",
    "\n",
    "$x_0$ | $x_1$ |...| $x_m$| $\\tilde y$\n",
    "-------------|--|-------------|----------------\n",
    "1 | $x_1^1$ | ... | $x_m^1$ | $y^1$\n",
    "1 | $x_1^2$ | ...| $x_m^2$ | $y^2$\n",
    "... | ... | ...| ... | ...\n",
    "\n",
    "Цель -- настроить веса нейрона $w_{0i}, w_{1i}, \\dots w_{mi}$ так, чтобы примеры из задачника были решены верно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Правило Розенблатта\n",
    "На шаге $t$ будем подавать очередной пример задачника и корректировать веса согласно формуле:\n",
    "\\begin{equation}\n",
    "    w_{ji}(t+1)= w_{ji}(t) + \\alpha x_i \\tilde{y}_j,\n",
    "\\end{equation}\n",
    "\n",
    "где $w_{ji}$ -- величина корректируемой синаптической связи, $t$ -- время, $x_i$ -- сигнал поданный в нейрон,  $\\tilde{y}_j$ -- желаемый отклик нейрона, $\\alpha$ -- параметр скорости обучения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Интуитивное объяснение:\n",
    "\n",
    "Для простоты возьмем сеть с двумя входами ($y = f(1 \\cdot w_0 + x_1w_1 + x_2w_w)$). Будем подавать примеры по одному и корректировать веса также по одному (если пример решен правильно, корректировать не нужно). Допустим, что на данном шаге корректируется вес $w_1$. Тогда на данной итерации зафиксируем остальные веса.\n",
    "\n",
    "* Пусть желаемый ответ $\\tilde y = 1$. Сеть решила пример неверно: $const + x_1 w_1 + const < 0$ => нужно увеличить $x_1 w_1$ за счет изменения веса. Тогда:  \n",
    "\\begin{equation}\n",
    "\\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "        x_1>0 =>  & w_1 \\mbox{ увеличиваем;} & \\mbox{ замечаем, что } \\tilde y x_1 >0\\\\\n",
    "        x_1<0 =>  & w_1 \\mbox{ уменьшаем;} & \\mbox{ замечаем, что } \\tilde y x_1 < 0\n",
    "    \\end{array}  \\right. . \n",
    "\\end{equation}\n",
    "* Пусть желаемый ответ $\\tilde y = -1$. Сеть решила пример неверно: $const + x_1 w_1 + const > 0$ => нужно уменьшить $x_1 w_1$ за счет изменения веса. Тогда:  \n",
    "\\begin{equation}\n",
    "\\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "        x_1>0 =>  & w_1 \\mbox{ уменьшаем;} & \\mbox{ замечаем, что } \\tilde y x_1 <0\\\\\n",
    "        x_1<0 =>  & w_1 \\mbox{ увеличиваем;} & \\mbox{ замечаем, что } \\tilde y x_1 >0\n",
    "    \\end{array}  \\right. . \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Алгоритм обучения\n",
    "\n",
    "1. Все веса сети приравниваются к нулю или генерируются датчком случайных чисел. Задается параметр скорости обучения $\\alpha$.\n",
    "2. На вход сети подается очередной пример $x$ из задачника и расчитываются выходы сети $y$.\n",
    "3. Если сеть вернула ошибочное значение ($y \\neq \\tilde{y}$), то производится коррекция весов:\n",
    "\\begin{equation*}\n",
    "    w_{ji}(t+1)= w_{ji}(t) + \\alpha x_i \\tilde{y}_j,\n",
    "\\end{equation*}\n",
    "Если ответ сети верен, то веса остаются прежними:\n",
    "\\begin{equation*}\n",
    "    w_{ji}(t+1)= w_{ji}(t)\n",
    "\\end{equation*}\n",
    "4. Алгоритм продолжается до тех пор, пока все примеры задачника не будут верно обработаны сетью (пока веса не перестанут меняться)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Очень важно.** Если существует решение задачи (набор весов), то перцептрон обучается за конечное число шагов.\n",
    "\n",
    "*Вопрос о том, при каких условиях решение сущетсвует, рассматривается чуть поздее.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вопрос.\n",
    "Можно ли при обучении по правилу Розенблатта использовать следующую функцию активации:\n",
    "\\begin{equation*}\n",
    "    y = f(u)= \\left\\lbrace \n",
    "        \\begin{array}{rl}\n",
    "            1, & \\mbox{если }u > 0\\\\\n",
    "            0, & \\mbox{если }u \\leq 0\n",
    "        \\end{array}  \\right. \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Правило обучения Видроу-Хоффа\n",
    "\n",
    "Используется для нейронов с линейной функцией активации:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(u) = k\\cdot u\n",
    "\\end{equation}\n",
    "\n",
    "*Для упрощения дальнейших выкладок примем $k=1$, на ход рассуждений это не влияет.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обучение: идея\n",
    "\n",
    "* Обучение с учителем.\n",
    "* Возьмем $L$ примеров из задачника. Рассчитаем для них выходы. Далее можно рассчитать общую ошибку:\n",
    "\\begin{equation}\n",
    "    E = \\sum_{i=1}^{L} E(i) = \\frac{1}{2}\\sum_{i=1}^{L} \\left( y(i) - \\tilde y (i)\\right)^2\n",
    "\\end{equation}\n",
    "где $y(i)$ -- выходное значение нейрона для примера $i$, $\\tilde y (i)$ -- желаемое значение (эталон) для этого примера.\n",
    "* При обучении будем менять веса нейрона так, чтобы ошибка уменьшалась.\n",
    "\n",
    "PS Почему нельзя считать ошибку по формуле:\n",
    "\\begin{equation}\n",
    "    E = \\sum_{i=1}^{L} E(i) = \\frac{1}{2}\\sum_{i=1}^{L} \\left( y(i) - \\tilde y (i)\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Упрощение\n",
    "Для облегчения задачи положим $L=1$, тогда ошибку будем рассчитывать для каждого примера в отдельности:\n",
    "\\begin{equation*}\n",
    "    E = \\frac{1}{2} \\left( y - \\tilde y \\right)^2\n",
    "\\end{equation*}\n",
    "Распишем $y$ через входные значения $X=(x_1,\\dots, x_n)$ нейрона и его весовые коэффициенты:\n",
    "\\begin{equation*}\n",
    "    E = \\frac{1}{2} \\left( \\sum_{i=0}^n w_i x_i - \\tilde y \\right)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Формулировка задачи\n",
    "Зафиксируем пример, на основе которого производится обучение, т.е. входные значения $X=(x_1,\\dots, x_n)$ и желаемое выходное значение $\\tilde y (i)$. Тогда ошибка сети для данного примера рассчитывается по формуле:\n",
    "\n",
    "\\begin{equation}\n",
    "    E = \\frac{1}{2} \\left( \\sum_{i=0}^n w_i x_i - \\tilde y \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "**Требуется** так изменить веса нейрона $w_0,\\dots, w_n$, чтобы величина ошибки для данного примера уменьшилась.\n",
    "\n",
    "*Сформулированная задача -- типичная задача на поиск экстремума.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/3dSurf.png\" height=\"10%\">\n",
    "\n",
    "В качестве осей координат у нас веса сети $w_{0}, w_{1}, \\dots w_{n}$, поверхность -- величина ошибки при конкретных значениях весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод градиентного спуска\n",
    "\n",
    "Задана функция нескольких переменных:\t$f = f(x_1,x_2,\\dots , x_n)$, требуется найти ее минимум.\n",
    "\n",
    "Будем осуществлять поиск в направлении наискорейшего спуска, которое задается вектором: $-\\nabla f$.\n",
    "\n",
    "Т.е. к точке минимума $x^* = (x_1^*, x_2^*,\\dots, x_n^*)$ будем приближаться итерационно, начиная с произвольной точки $x^0 = (x_1^0, x_2^0,\\dots, x_n^0)$. Каждое следующее приближение будем находить по формуле:\n",
    "\\begin{equation}\\label{eq:grad_metod}\n",
    "    x^{k+1} = x^{k} - \\alpha \\nabla f(x^k),\n",
    "\\end{equation}\n",
    "где $\\alpha$ --- параметр, регулирующий величину шага.\n",
    "\n",
    "Формула покоординатно расписывается следующим образом:\\pause\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        x^{k+1}_1 = x^{k}_1 - \\alpha \\frac{\\partial f}{\\partial x_1}\\Big|_{(x=x^k)},\\\\\n",
    "        \\dots \\\\\n",
    "        x^{k+1}_n = x^{k}_n - \\alpha \\frac{\\partial f}{\\partial x_n}\\Big|_{(x=x^k)}\n",
    "    \\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Решение\n",
    "Воспользуемся методом градиентного спуска. Тогда весовые коэффициенты нейрона должны измениться согласно следующей формуле:\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        w_{0}(t+1) = w_0(t) - \\alpha \\frac{\\partial E}{\\partial w_0(t)},\\\\\n",
    "        \\dots,\\\\\n",
    "        w_{j}(t+1) = w_j(t) - \\alpha \\frac{\\partial E}{\\partial w_j(t)},  \\\\\n",
    "        \\dots,\\\\\n",
    "        w_{n}(t+1) = w_n(t) - \\alpha \\frac{\\partial E}{\\partial w_n(t)}\n",
    "    \\end{split}\t\t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Производная величины $E$\n",
    "Вычислим производные $\\frac{\\partial E}{\\partial w_j(t)}$: \n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial E}{\\partial w_j} = \\frac{\\partial \\left[\\frac{1}{2} \\left( \\left(\\sum_{i=0}^n w_i x_i \\right) - \\tilde y \\right)^2\\right]}{\\partial w_j}\n",
    "\\end{equation*}\n",
    "здесь $x_i=const$, $\\tilde y = const$\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial E}{\\partial w_j} = \n",
    "        \\left( \\left(\\sum_{i=0}^n w_i x_i \\right) - \\tilde y \\right) \\frac{\\partial \\left( \\left(\\sum_{i=0}^n w_i x_i \\right) - \\tilde y \\right)}{\\partial w_j}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E}{\\partial w_j} = ( y - \\tilde y ) x_j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Дельта-правило\n",
    "Таким образом получили, что правило обучения Видроу-Хоффа (дельта-правило) должно быть записано следующим образом:\n",
    "\n",
    "*Пусть $w=(w_0,w_1,\\dots, w_n)$ -- вектор весовых коэффициентов нейрона, $x=(x_1,\\dots, x_n)$ -- входные значения нейрона, а $\\tilde y$ - желаемое выходное значение, соответствующее заданным входам. Тогда весовые коэффициенты сети следует изменять согласно следующей формуле:*\n",
    "\n",
    "\\begin{equation}\\label{eq:delta}\n",
    "    w_j(t+1) = w_j(t) - \\alpha ( y - \\tilde y ) x_j,\n",
    "\\end{equation}\n",
    "где $t$ - номер итерации, $\\alpha\\in(0,1)$ --- некоторый параметр (скорость обучения).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обучения Видроу-Хоффа\n",
    "\n",
    "1. Составляется задачник и задаются параметры: \n",
    "    * скорость обучения $\\alpha\\in(0,1)$ \n",
    "    * устраивающая точность, т.е. ошибка $E_{good}$, которую нужно достичь в процессе обучения\n",
    "    \n",
    "2. Случайным образом инициализируются весовые коэффициенты и порог сети.\n",
    "3. На входы сети подаются входные образы $x=(x_1,\\dots, x_n)$ и вычисляются выходные значения сети $y$.\n",
    "4. Осуществляется коррекция весовых коэффициентов согласно дельта-правилу.\n",
    "5. Алгоритм продолжается до тех пор, пока суммарная среднеквадратичная ошибка сети не станет меньше заданной: $E < E_{good}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Сравнение правил Розенблатта и Видроу-Хоффа\n",
    "\n",
    "* Правило обучения Розенблатта \t\t\t\n",
    "\\begin{equation*}\n",
    "    w_j(t+1) = w_j(t) + \\alpha \\tilde y x_j,\n",
    "\\end{equation*}\n",
    "Обучение производится при $y \\neq \\tilde y$, при этом: $y \\in \\{-1,1\\}$ $\\tilde y \\in \\{-1,1\\}$.\n",
    "\n",
    "* Правило обучения Видроу-Хоффа\n",
    "\\begin{equation*}\n",
    "    w_j(t+1) = w_j(t) - \\alpha ( y - \\tilde y ) x_j,\n",
    "\\end{equation*}\n",
    "\n",
    "**Оба правила обучения можно записать в общей форме:**\n",
    "\\begin{equation*}\n",
    "        w_j(t+1) = w_j(t) - \\alpha ( y - \\tilde y ) x_j,\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Общее правило обучения\n",
    "1. Составляется задачник и задаются параметры: \n",
    "    * скорость обучения $\\alpha\\in(0,1)$;\n",
    "    * формулируется критерий, согласно которому останавливается обучение.\n",
    "    \n",
    "2. Случайным образом инициализируются весовые коэффициенты и порог сети.\n",
    "3. На входы сети подаются *случайно выбираемые* входные образы $x=(x_1,\\dots, x_n)$ из задачинка и вычисляются выходные значения сети $y$.\n",
    "4. Осуществляется коррекция весовых коэффициентов согласно дельта-правилу:\n",
    "    \\begin{equation*}\n",
    "        w_j(t+1) = w_j(t) - \\alpha ( y - \\tilde y ) x_j,\n",
    "\\end{equation*}\n",
    "5. Алгоритм продолжается до тех пор, пока не достигнут критерий останова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Выбор параметра скорости\n",
    "Слишком маленькая скорость | Слишком большая скорость\n",
    "-------------------------------|------------------\n",
    "<img src=\"img/alpha1.png\" height=\"10%\">|<img src=\"img/alpha2.png\" height=\"10%\">\n",
    "\n",
    "Параметр скорости обучения $\\alpha$ должен быть достаточно большим, иначе процесс коррекции весов будет <<топтаться на месте>> и  должен быть достаточно маленьким, иначе процесс коррекции весов может оказаться неустойчивым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Как же тогда выбирать $\\alpha$?\n",
    "\n",
    "* Можно \"методом тыка\" (не получается с одним параметром, пробуем другой)\n",
    "* Можно выбирать $\\alpha$, которое будет уменьшаться в процессе обучения, например:\n",
    "\\begin{equation*}\n",
    "    \\alpha (t) = \\frac{1}{t},\n",
    "\\end{equation*}\n",
    "где $t$ -- номер итерации. (Тут возможны проблемы...)\n",
    "* Можно использовать адаптивный шаг обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Адаптивный шаг обучения\n",
    "*Адаптивный шаг обучения* -- такой параметр $\\alpha(t)$, который целенаправленно выбирается на каждом шаге алгоритма таким образом, чтобы минимизировать среднеквадратичную ошибку сети.\n",
    "\n",
    "Для линейной нейронной сети значение адаптивного шага обучения вычисляется на основе выражения:\n",
    "\\begin{equation}\n",
    "     \\alpha(t) = \\frac{1}{1 + \\sum_{i=1}^n x_i^2(t)} \n",
    " \\end{equation} \n",
    " \n",
    "(Но нужно признаться, что это работает только в простых случаях)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Нейрон как классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Задача классификации:* формализованная задача, в которой имеется множество объектов (ситуаций), разделенных некоторым образом на классы. Задано конечное множество объектов, для которых известно, к каким классам они относятся. Это множество называется выборкой. Классовая принадлежность остальных объектов не известна. Требуется построить алгоритм, способный классифицировать произвольный объект из исходного множества.\n",
    "\n",
    "*Классифицировать объект:* значит, указать номер (или наименование) класса, к которому относится данный объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Пусть в $n$-мерном пространстве задано два класса $C_1$ и $C_2$.\n",
    "\n",
    "<img src=\"img/class.png\" height=\"10%\">\n",
    "\n",
    "\n",
    "Рассмотрим нейрон с пороговой функцией активации:\n",
    "\\begin{equation}\n",
    "    y = f(u) = \\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "    1, & \\mbox{если $u \\geq 0$}\\\\\n",
    "    -1, & \\mbox{если u<0}\n",
    "    \\end{array}  \\right. .\n",
    "\\end{equation}\n",
    "\n",
    "Можно ли обучить нейрон так, чтобы \n",
    "* $y=1$ для любого $x \\in C_1$.\n",
    "* $y=-1$ для любого $x \\in C_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Оказывается, настроить веса нейрона так, чтобы он позволял разделить два произвольных класса невозможно.\n",
    "\n",
    "Чтобы однослойный перцептрон функционировал корректно, два класса $C_1$ и $C_2$ должны быть линейно разделимыми.\n",
    "\n",
    "Линейно-разделимые классы| Линейно-неразделимые классы\n",
    "-------------------------------|------------------\n",
    "<img src=\"img/lin_razd.png\" height=\"10%\">|<img src=\"img/lin_ne_razd.png\" height=\"10%\">\n",
    "\n",
    "\n",
    "Если классы линейно-разделимы, то существует *разделяющая линия*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Почему перцептрон может работать только с линейно-разделимыми классами?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Нейрон выдает ответ ($+1$ или $-1$) в зависимости от того, какое значение потенциала $u=\\sum_{i=0}^n w_i x_i$ было получено при заданных входах $x=x_1,\\dots x_n$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    y = f(u) = \\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "    1, & \\mbox{если }u > 0\\\\\n",
    "    -1, & \\mbox{если }u<0\n",
    "    \\end{array}  \\right. .\n",
    "\\end{equation*}\n",
    "\n",
    "Таким образом при $u = 0$ происходит резкий переход значения $y$ от +1 к -1.\n",
    "\n",
    "\n",
    "*Какая фигура в пространстве задается уравнением $u=0$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Уравнение $u=\\sum_{i=0}^n w_i x_i=0$ задает:\n",
    "\n",
    "* $n=2$: линию на плоскости: $w_0 + w_1x_1 + w_2x_2=0$\n",
    "* $n=3$: плоскость в пространстве: $w_0 + w_1x_1 + w_2x_2 + w_3 x_3=0$\n",
    "* $n>3$: гиперплоскость в многомерном пространстве: $w_0 + w_1x_1 + w_2x_2 + w_3 x_3 +\\dots +w_n x_n=0$\n",
    "\n",
    "Линия (плоскость, гиперплоскость) делит плоскость (пространство) на две полуплоскости (два полупространства): положительную и отрицательную."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Многослойный перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![Многослойный перцептрон](img/net.png)\n",
    "\n",
    "Многослойный перцептрон состоит из нескольких слоев нейронов (входной слой (нулевой), первый скрытый слой, второй скрытый слой, ..., выходной слой).\n",
    "\n",
    "* нейроны первого слоя получают входные сигналы, преобразуют их и передают нейронам второго слоя;\n",
    "* далее срабатывает второй слой, получающий сигналы от первого, этот слой также производит преобразование сигналов и их дальнейшую передачу третьему слою и т.д.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Особенности многослойного перцептрона\n",
    "\n",
    "Обычно:\n",
    "\n",
    "* Каждый нейрон имеет нелинейную функцию активации (почему нельзя линейную?).\n",
    "* Функция активации является всюду дифференцируемой (но в современных реализациях это не обязательно).\n",
    "* Несколько скрытых слоев.\n",
    "\n",
    "В первых работах очень часто использовалась сигмоидальная функция активации:\n",
    "\\begin{equation}\n",
    "    f(u) = \\frac{1}{1+ e^{-u}}\n",
    "\\end{equation}\n",
    "\n",
    "Сейчас часто используется ReLU:\n",
    "\\begin{equation}\n",
    "f(u)= \\left\\lbrace \n",
    "    \\begin{array}{rl}\n",
    "        0, & \\mbox{если $u < 0$}\\\\\n",
    "        u, & \\mbox{если $u \\geq 0$}\n",
    "    \\end{array}   \\right. \n",
    "\\end{equation}\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Обучение многослойного перцептрона\n",
    "\n",
    "Вспоминаем, как производилось обучение однослойного перцептрона.\n",
    "\n",
    "* Обучение с учителем.\n",
    "* Возьмем $L$ примеров из задачника. Рассчитаем для них выходы. Далее можно рассчитать общую ошибку:\n",
    "\\begin{equation}\n",
    "    E = \\sum_{i=1}^{L} E(i)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E(i) = \\frac{1}{2}\\sum_{i=1}^{L} \\left( y(i) - \\tilde y (i)\\right)^2\n",
    "\\end{equation}\n",
    "где $y(i)$ -- выходное значение нейрона для примера № $i$, $\\tilde y (i)$ -- желаемое значение (эталон) для примера $i$.\n",
    "* Процедура обучения состоит в пошаговом подборе весов нейрона таким образом, чтобы ошибка $E$ уменьшалась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обучение однослойной сети: дельта-правило\n",
    "\n",
    "Пусть $w=(w_0,w_1,\\dots, w_m)$ --- вектор весовых коэффициентов нейрона, $x=(x_1,\\dots, x_m)$ --- входные значения нейрона, а $\\tilde y$ --- желаемое выходное значение, соответствующее заданным входам. Тогда весовые коэффициенты сети следует изменять согласно следующей формуле:\n",
    "\\begin{equation}\n",
    "    w_j(t+1) = w_j(t) - \\alpha ( y - \\tilde y ) x_j,\n",
    "\\end{equation}\n",
    "где $t$ -- номер итерации, $\\alpha\\in(0,1)$ -- некоторый параметр (скорость обучения)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Что мешает обучать многослойный перцептрон используя выведенное ранее дельта-правило?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "У однослойной сети ошибки зависят только от одного слоя нейронов. Поэтому зная ответ сети и желаемый отклик из задачника можно подсчитать общую ошибку сети и рассчитать, как должны меняться весовые коэффициенты сети.\n",
    "\n",
    "У многослойной сети мы можем вычислить ошибки только для выходного слоя, но общая ошибка сети зависит от весовых коэффициентов всех слоев сети.\n",
    "\n",
    "**Мы не знаем, как нужно корректировать веса скрытых слоев сети, так как не знаем, насколько сильную ошибку вносят внутренние слои в общую ошибку.**\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Алгоритм обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Общая идея\n",
    "Обучение будет производиться также по схеме обучения с учителем, когда процедуре обучения передается два параметра: входные значения сети $(x_1, x_2, \\dots, x_m)$ и ожидаемые выходы $(\\tilde{y_1}, \\tilde{y_2}, \\dots, \\tilde{y_n})$, соответствующие заданным входам. \n",
    "\n",
    "Далее рассчитываются реальные выходные значения $(y_1, y_2, \\dots, y_n)$, которые получаются, если на входы сети подать заданные $(x_1, x_2, \\dots, x_m)$. В результате можно рассчитать общую ошибку работы сети для данного примера:\n",
    "\\begin{equation}\n",
    "E=\\frac12 \\sum_{i=1}^n (\\tilde{y_i}-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "Для того, чтобы уменьшить ошибку $E$, можно воспользоваться градиентными методами оптимизации.\n",
    "\n",
    "*(пока ничего нового не появилось)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Расчет ошибок\n",
    "Рассмотрим пример сети с одним скрытым слоем:\n",
    "\n",
    "<img src=\"img/net_mkn.png\" height=\"30%\">\n",
    "\n",
    "Рассчитаем выходные значения нейронов скрытого слоя $y^1_i$:\n",
    "\\begin{equation}\n",
    "    y^{(1)}_i = f(\\sum_{p=0}^m w_{ip}^0 x_p)\n",
    "\\end{equation}\n",
    "где $w_{ip}^0$ --- весовые коэффициенты нейрона $i$ скрытого слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/net_mkn.png\" height=\"30%\">\n",
    "\n",
    "Пусть уже рассчитаны выходные значения нейронов скрытого слоя $y^{(1)}_i$. Тогда выходной сигнал $j$-го нейрона выходного слоя рассчитывается по формуле:\n",
    "\\begin{equation}\n",
    "    y^{(2)}_j = f(\\sum_{i=0}^K w_{ji}^1 y_i^{(1)})\n",
    "\\end{equation}\n",
    "где $w_{ji}^1$ --- весовые коэффициенты нейронов выходного слоя, $j=1,2,\\dots,N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/net_mkn.png\" height=\"30%\">\n",
    "\n",
    "\n",
    "Таким образом, выходное значение сети рассчитывается по формуле:\n",
    "\n",
    "\\begin{equation}\n",
    "    y^{(2)}_j = f(\\sum_{i=0}^K w_{ji}^1 y_i^{(1)}) = f(\\sum_{i=0}^K w_{ji}^1 \\cdot (f(\\sum_{p=0}^m w_{ip}^0 x_p)) )\n",
    "\\end{equation}\n",
    "\n",
    "Аналогично можно рассчитать выходное значение сети с двумя, тремя и т.д. скрытыми слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вывод\n",
    "\n",
    "Таким образом ошибка сети для конкретного примера из задачника может быть рассчитана по формуле:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    E = \\frac12 \\sum_{j=1}^N \\left ( y^{(2)}_j - \\tilde y_j \\right )^2 = \\\\\n",
    "    = \\frac12  \\sum_{j=1}^N \\left (f \\left (\\sum_{i=0}^K w_{ji}^1 \\cdot \\left(f(\\sum_{p=0}^m w_{ip}^0 x_p)\\right) \\right) - \\tilde y_j \\right ) ^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "В этой формуле фигурируют веса всех нейронов сети, а не только нейронов выходного слоя. Следовательно, формула дает возможность рассчитать вклад в общую ошибку каждого весового коэффициента в отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Коррекция весов\n",
    "\n",
    "\n",
    "Итак, при обучении будем корректировать веса нейронов так, чтобы ошибка сети уменьшалась.\n",
    "\\begin{equation}\n",
    "    E = \\sum_{j=1}^N \\left ( y^{(2)}_j - \\tilde y_j \\right )^2 \\to \\min\n",
    "\\end{equation}\n",
    "\n",
    "Для этого воспользуемся градиентными методами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения: <<взгляд сверху>>\n",
    "\n",
    "Выберем очередной пример из задачника.\tАлгоритм будем выполнять в два этапа:\n",
    "\n",
    "* Прямой проход, во время которого рассчитываются отклики каждого слоя сети, начиная с первого и заканчивая последним, выходным, слоем.\n",
    "* Обратный проход, во время которого рассчитывается ошибка для каждого слоя сети, начиная с последнего (выходного) слоя и заканчивая первым слоем сети.\n",
    "\n",
    "После расчета ошибок производится коррекция весов, для того, чтобы уменьшить величину ошибки $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 0. Инициализация\n",
    "    \n",
    "* Составляем задачник.\n",
    "* Выбираем архитектуру сети (число слоев, число нейронов в слоях, функцию активации нейронов).\n",
    "* Генерируем синаптические веса и пороговые значения нейронов с помощью датчика случайных чисел со средним 0.\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 1. Предъявление примеров обучения (прямой проход).\n",
    "\n",
    "Пусть пример представлен парой векторов $(x_1, x_2, \\dots, x_m)$ --- входные значения сети и $(\\tilde{y_1}, \\tilde{y_2}, \\dots, \\tilde{y_n})$ --- ожидаемые выходы.\n",
    "\n",
    "1. Вычисляем потенциалы нейронов и функциональные сигналы:\n",
    "\\begin{equation}\\label{potenc}\n",
    "u_j^q = \\sum_i w_{ji}^q y_i^{q-1}; \\qquad y^q_j=f_j(u_j^q)\n",
    "\\end{equation}\n",
    "где $y_i^{q-1}$ -- выходной сигнал нейрона $i$, расположенного в предыдущем слое, $w_{ji}^q$ -- вес связи нейрона $j$ слоя $q$ с нейроном $i$ слоя $q-1$ (для $i=0$ считаем, что $y_0^{l-1}=1$, и $w_{j0}^q=b^q_j$ -- порог);  $f_j$ -- функция активации нейрона $j$. Здесь для удобства записи принято, что если нейрон находится в первом скрытом слое сети (т.е $q=1$), то считаем, что $y^0_j=x_j$.\n",
    "2. Вычисляем сигнал ошибки сети:\n",
    "\\begin{equation}\n",
    "    e_j=\\tilde{y}_j - y_j^Q\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 2. Представление примеров обучения (обратный проход).\n",
    "    \n",
    "3. Вычисляем локальные градиенты узлов сети по формуле:\n",
    "\\begin{equation}\\label{grad_main}\n",
    "    \\delta_j^q= \\left\\{ \n",
    "        \\begin{array}{ll}\n",
    "         e_j^Q f'_j(u_j) & \\mbox{для нейрона $j$ выходного слоя  $Q$} \\\\ \n",
    "         f'_j(u_j^q)\\sum_k \\delta^{q+1}_k w_{kj}^{q+1} & \\mbox{для нейрона $j$ скрытого слоя $q$}\n",
    "        \\end{array}  \\right .\n",
    "\\end{equation}\n",
    "4. Корректируем веса:\n",
    "\\begin{equation}\\label{korr_w}\n",
    "w_{ji}^q(t+1) = w_{ji}^q(t) + \\alpha w_{ji}^q(t-1) + \\eta \\delta^q_j(t) y^{q-1}_i(t)\n",
    "\\end{equation}\n",
    "где $t$ --- номер итерации, $\\alpha \\in [0,1)$ и $\\eta \\in (0,1)$ --- параметры, влияющие на скорость градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: 3. Итерации\n",
    "\n",
    "Последовательно выполняем прямой и обратный проходы, предъявляя сети все примеры обучения, пока не будет достигнут критерий останова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: Параметры скорости и момента\n",
    "    \n",
    "При коррекции весов используется параметр момента $\\alpha$  и скорость обучения $\\eta$. \n",
    "\\begin{equation*}\n",
    "    w_{ji}^q(t+1) = w_{ji}^q(t) + \\alpha w_{ji}^q(t-1) + \\eta \\delta^q_j(t) y^{q-1}_i(t)\n",
    "\\end{equation*}\n",
    "\n",
    "* Чем меньше параметр скорости $\\eta$ => тем меньше корректировка весов => тем более гладкой будет траектория изменения весов $w_{ji}^q$. Но это улучшение происходит за счет замедления обучения.\n",
    "* Если увеличить $\\eta$ для повышения скорости => большие изменения весов => возможно неустойчивое состояние системы.\n",
    "\n",
    "Вводится момент $\\alpha$ => ускорение обучения, если веса изменяются в одном направлении, и стабилизация, если веса сети менялись в разных направлениях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм обратного распространения ошибки: Критерии останова\n",
    "\n",
    "Критериев может быть много и разных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Обучение останавливается при достижении такого состояния, когда сеть становится способна обобщать примеры, предъявленные ей в прошлом, на другие, незнакомые примеры.\n",
    "\n",
    "\n",
    "Для определения было ли достигнуто данное состояние используется процедура перекрестной проверки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Перекрестная проверка\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Явление переобучения\n",
    "<img src=\"img/trolernado.png\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Перекрестная проверка: классический вариант\n",
    "\n",
    "\n",
    "* Данные случайным образом разбивают на обучающее множество и тестовое множество. Обучающее множество, в свою очередь, разбивают на два подмножества: оценочное подмножество и проверочное (валидационное) подмножество.\n",
    "* Сеть обучают на оценочном подмножестве. \t\n",
    "Чтобы избежать переобучения, сеть постоянно проверяют, предъявляя ей незнакомые примеры из проверочного подмножества. (Обычно суммарная ошибка сначала падает, потом начинает возрастать).\n",
    "* Проверочное множество также участвовало в процедуре обучения и в результате может оказаться, что сеть была переобучена на проверочном множестве. Поэтому используют третье, тестовое множество, которое не участвовало в процедуре обучения. На этом множестве и проверяют качество работы сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Глубокие сети против широких сетей\n",
    "\n",
    "<img src=\"img/layers_and_out.png\" height=\"30%\">\n",
    "(Гудфеллоу Я. и др. \"Глубокое обучение\").\n",
    "\n",
    "* Нейроны с функцией активации \"модуль\": $f(u) = |u|$.\n",
    "* Нейросеть с большим числом слоев может уловить любые регулярные (повторяющиеся) шаблоны.\n",
    "\n",
    "Слева: блок \"модуль\" порождает одинаковые выходы для любой пары зеркально симметричных входов (ось симметрии задается весами и смещением блока).\n",
    "\n",
    "Центр: добавление второго скрытого слоя реализует еще один перегиб перегиб пространства по новой оси семетрии.\n",
    "\n",
    "Справа: добавление третьего слоя позволяет получить следующий перегиб пространства и получить еще одну симметрию.\n",
    "\n",
    "Первый блок ищет простые шаблоны, второй - двукратные, а третий - четырехкратные повторения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
